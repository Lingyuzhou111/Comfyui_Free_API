# -*- coding: utf-8 -*-
"""
LLM Prompt Enhance Node

åŠŸèƒ½:
- ä» ComfyUI/custom_nodes/Comfyui_Free_API/config.json çš„ "LLM" æ®µè¯»å–æä¾›æ–¹ã€base_urlã€api_keyã€model åˆ—è¡¨
- åœ¨ ComfyUI ä¸­æä¾›ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå…è®¸ç”¨æˆ·é€‰æ‹© "llm_model"ï¼ˆå±•ç¤ºä¸º "æä¾›æ–¹:å‹å·"ï¼‰
- è¯»å– Prompt_Enhance_Node/llm_sys_prompt.json ä¸­çš„ç³»ç»Ÿæç¤ºè¯æ¨¡ç‰ˆåç§°ä½œä¸ºä¸‹æ‹‰é€‰é¡¹ï¼ˆè‹¥æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œåˆ™æä¾›å†…ç½®é»˜è®¤æ¨¡ç‰ˆï¼‰
- æ¥æ”¶ç”¨æˆ· user_promptã€max_tokensã€temperatureã€top_pï¼›å½“ preset_template é€‰æ‹©â€œæ‰‹åŠ¨è¾“å…¥â€æ—¶ï¼Œä½¿ç”¨ sys_prompt æ–‡æœ¬æ¡†å†…å®¹ä½œä¸ºç³»ç»Ÿæç¤ºè¯
- æŒ‰ OpenAI å…¼å®¹ Chat Completions æ¥å£æ ¼å¼è°ƒç”¨å¯¹åº” base_urlï¼ˆéœ€ /chat/completionsï¼‰ï¼Œè¿”å›å¢å¼ºåçš„æç¤ºè¯
- è¾“å‡º:
  1) context: å±•ç¤ºæœ€ç»ˆæäº¤ç»™ API çš„ system_prompt + user_promptï¼ˆä¾¿äºæ£€æŸ¥ä¸Šä¸‹æ–‡ï¼‰
  2) result: API å“åº”ç”Ÿæˆå†…å®¹ï¼ˆå¢å¼ºåçš„æç¤ºè¯ï¼‰

æ³¨æ„:
- æœ¬å®ç°ä»…ç”¨äºåˆæ³•ã€æ­£å½“çš„æç¤ºè¯ä¼˜åŒ–å·¥ä½œï¼Œä¸åŒ…å«ä»»ä½•æ¶æ„ç”¨é€”ã€‚
- ä¸¥æ ¼é¿å…æ³„éœ²ç³»ç»Ÿä¿¡æ¯ä¸å†…éƒ¨é…ç½®ã€‚
"""

import json
import os
import re
import time
from typing import Dict, Any, List, Tuple, Optional

try:
    import requests
except Exception:
    requests = None

# è¯»å–é…ç½®å·¥å…·
def _load_config(config_path: str) -> Dict[str, Any]:
    try:
        if not os.path.exists(config_path):
            return {}
        with open(config_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if not content:
                return {}
            return json.loads(content)
    except Exception:
        return {}

# è¯»å–ç³»ç»Ÿæ¨¡ç‰ˆå·¥å…·
def _load_sys_templates(templates_path: str) -> Dict[str, str]:
    """
    æœŸæœ›æ–‡ä»¶æ ¼å¼:
    {
      "æ¨¡æ¿ä¸­æ–‡åA": "è¿™é‡Œæ˜¯system promptå†…å®¹A",
      "æ¨¡æ¿ä¸­æ–‡åB": "è¿™é‡Œæ˜¯system promptå†…å®¹B"
    }
    è‹¥æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›é»˜è®¤æ¨¡ç‰ˆé›†åˆã€‚
    """
    defaults = {
        "é€šç”¨å¢å¼º:å›¾æ–‡æç¤ºè¯æ¶¦è‰²": (
            "ä½ æ˜¯èµ„æ·±AIGCæç¤ºè¯å·¥ç¨‹å¸ˆã€‚ç›®æ ‡:åœ¨ä¸æ”¹å˜æ„å›¾çš„å‰æä¸‹ï¼Œå°†ç”¨æˆ·ç”¨äºAIç»˜ç”»/è§†é¢‘ç”Ÿæˆçš„æç¤ºè¯è¿›è¡Œç»“æ„åŒ–ã€å…·ä½“åŒ–ä¸å¯æ§åŒ–ï¼Œ"
            "è¦æ±‚:\n"
            "1) æ˜ç¡®ä¸»ä½“/é£æ ¼/æ„å›¾/å…‰ç…§/é…è‰²/æè´¨/åˆ†è¾¨ç‡/è´Ÿé¢å…ƒç´ ã€‚\n"
            "2) ä¿ç•™ç”¨æˆ·å…³é”®å…ƒç´ ï¼Œæ¶ˆé™¤æ­§ä¹‰ï¼›è¡¥å……å¿…è¦ç»†èŠ‚è¯æ±‡ï¼›é¿å…å†—ä½™ä¸è‡ªç›¸çŸ›ç›¾æè¿°ã€‚\n"
            "3) åªè¾“å‡ºæœ€ç»ˆå¯ç”¨äºæ¨¡å‹çš„è‹±æ–‡promptæ–‡æœ¬ï¼ˆå¯ç”¨é€—å·/å¥å­ç»„ç»‡ï¼‰ï¼Œä¸æ·»åŠ è§£é‡Šã€‚"
        ),
        "å½±è§†å‘å¢å¼º:é•œå¤´è¯­è¨€å¼ºåŒ–": (
            "ä½ æ˜¯ç”µå½±åˆ†é•œæŒ‡å¯¼AIã€‚å°†æç¤ºè¯æ”¹å†™ä¸ºæ›´å…·é•œå¤´è¯­è¨€çš„è‹±æ–‡æè¿°ï¼ŒåŒ…å«é•œå¤´ç±»å‹ï¼ˆwide shotã€close-upï¼‰ã€"
            "æœºä½/ç§»åŠ¨ï¼ˆlow angle, dolly-inï¼‰ã€å…‰æ•ˆï¼ˆrim light, volumetric lightï¼‰ã€æ°›å›´/è‰²æ¸©ï¼›"
            "ä¿æŒåŸæ„å¹¶è¡¥å……å¯è§†åŒ–ç»†èŠ‚ã€‚ä»…è¾“å‡ºè‹±æ–‡æç¤ºè¯ã€‚"
        ),
        "è§’è‰²ç«‹ç»˜:ç»†èŠ‚åˆ»ç”»": (
            "ä½ æ˜¯è§’è‰²ç¾æœ¯æç¤ºå·¥ç¨‹å¸ˆã€‚å°†æç¤ºè¯æ”¹å†™ä¸ºè§’è‰²ç«‹ç»˜å‘çš„è‹±æ–‡æè¿°ï¼Œå¼ºè°ƒå¤–è§‚ï¼ˆå‘å‹ã€ç³è‰²ã€æœé¥°ã€é…ä»¶ï¼‰ã€"
            "é£æ ¼ï¼ˆanime/realistic/illustrationï¼‰ã€å§¿æ€ã€è¡¨æƒ…ã€èƒŒæ™¯ç®€æ´åº¦ã€ç”»è´¨å…³é”®è¯ã€‚ä¿æŒåŸæ„å¹¶æå‡å¯æ§æ€§ã€‚ä»…è¾“å‡ºè‹±æ–‡æç¤ºè¯ã€‚"
        ),
    }
    try:
        if not os.path.exists(templates_path):
            return defaults
        with open(templates_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if not content:
                return defaults
            data = json.loads(content)
            if isinstance(data, dict) and data:
                cleaned = {str(k): str(v) for k, v in data.items() if isinstance(k, str)}
                return cleaned or defaults
            return defaults
    except Exception:
        return defaults

def _build_llm_options(config: Dict[str, Any]) -> Tuple[List[str], Dict[str, Dict[str, str]]]:
    """
    è¿”å›:
    - options: ["æä¾›æ–¹:å‹å·", ...]
    - map_info: {"æä¾›æ–¹:å‹å·": {"base_url": "...", "api_key": "...", "model": "å‹å·"}}
    """
    options: List[str] = []
    map_info: Dict[str, Dict[str, str]] = {}
    if not config:
        return options, map_info

    llm = config.get("LLM", {})
    for provider, info in llm.items():
        try:
            base_url = str(info.get("base_url", "")).strip()
            api_key = str(info.get("api_key", "")).strip()
            models = info.get("model", [])
            if not isinstance(models, list):
                continue
            for m in models:
                model_name = str(m).strip()
                label = f"{provider}:{model_name}".replace("ï¼š", ":")
                options.append(label)
                map_info[label] = {"base_url": base_url, "api_key": api_key, "model": model_name}
        except Exception:
            continue
    return options, map_info

def _normalize_base_url(url: str) -> str:
    u = url.strip()
    u = re.sub(r"\s+", " ", u).strip()
    u = u.rstrip("/")
    return u

def _chat_completions(
    base_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    timeout: int = 60,
) -> Tuple[bool, str]:
    """
    OpenAI å…¼å®¹æ¥å£:
      POST {base_url}/chat/completions
    body:
      {"model": "...", "messages": [{"role":"system","content":...},{"role":"user","content":...}],
       "max_tokens":..., "temperature":..., "top_p":...}
    """
    if requests is None:
        return False, "requests åº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…åé‡è¯•ã€‚"

    try:
        url = _normalize_base_url(base_url) + "/chat/completions"
        headers = {"Content-Type": "application/json"}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"

        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "stream": False,
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)
        if resp.status_code >= 400:
            return False, f"HTTP {resp.status_code}: {resp.text}"

        data = resp.json()
        if isinstance(data, dict):
            choices = data.get("choices") or []
            if isinstance(choices, list) and choices:
                msg = choices[0].get("message") or {}
                content = msg.get("content")
                if isinstance(content, str) and content.strip():
                    return True, content.strip()
                for key in ["reasoning_content", "text", "output", "answer"]:
                    val = msg.get(key)
                    if isinstance(val, str) and val.strip():
                        return True, val.strip()
            for key in ["output", "answer", "text", "message"]:
                v = data.get(key)
                if isinstance(v, str) and v.strip():
                    return True, v.strip()
        return False, json.dumps(data, ensure_ascii=False)
    except Exception as e:
        return False, f"Exception: {e}"

class LLM_Prompt_Enhance_Node:
    """
    ComfyUI èŠ‚ç‚¹å®šä¹‰:
    - è¾“å…¥:
        1) llm_model: ä¸‹æ‹‰ï¼Œæ¥è‡ª config.json çš„ LLM ç»„åˆé¡¹ ("æä¾›æ–¹:å‹å·")
        2) preset_template: ä¸‹æ‹‰ï¼Œé¦–ä½ä¸ºâ€œæ‰‹åŠ¨è¾“å…¥â€ï¼Œå…¶ä½™æ¥è‡ª llm_sys_prompt.jsonï¼›å¦‚æœæ–‡ä»¶ä¸ºç©ºåˆ™æä¾›å†…ç½®æ¨¡ç‰ˆ
        3) sys_prompt: æ–‡æœ¬æ¡†ï¼ˆsys_promptï¼Œè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ä»…åœ¨é¢„è®¾æ¨¡æ¿ä¸ºâ€œæ‰‹åŠ¨è¾“å…¥â€æ—¶æ‰ç”Ÿæ•ˆï¼‰
        4) user_prompt: æ–‡æœ¬æ¡†
        5) max_tokens: æ•´æ•°ï¼ˆé»˜è®¤ 2048ï¼‰
        6) temperature: æµ®ç‚¹ï¼ˆé»˜è®¤ 0.8ï¼‰
        7) top_p: æµ®ç‚¹ï¼ˆé»˜è®¤ 0.6ï¼‰
    - è¾“å‡º:
        1) context: strï¼ˆsystem + user æœ€ç»ˆæäº¤å†…å®¹ï¼‰
        2) result: strï¼ˆAPI å“åº”å¢å¼ºç»“æœï¼‰
    """

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ è½½é…ç½®ä¸æ¨¡ç‰ˆ
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # custom_nodes/Comfyui_Free_API
        config_path = os.path.join(base_dir, "config.json")
        templates_path = os.path.join(os.path.dirname(__file__), "llm_sys_prompt.json")

        config = _load_config(config_path)
        llm_options, _ = _build_llm_options(config)
        if not llm_options:
            llm_options = []

        sys_templates = _load_sys_templates(templates_path)
        template_names = ["æ‰‹åŠ¨è¾“å…¥"] + (list(sys_templates.keys()) if sys_templates else ["é€šç”¨å¢å¼º:å›¾æ–‡æç¤ºè¯æ¶¦è‰²"])

        return {
            "required": {
                "llm_model": (llm_options if llm_options else ["æœªæ£€æµ‹åˆ°æ¨¡å‹é…ç½®"],),
                "preset_template": (template_names,),
                "sys_prompt": ("STRING", {"multiline": True, "default": "è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ä»…åœ¨é¢„è®¾æ¨¡æ¿ä¸ºâ€œæ‰‹åŠ¨è¾“å…¥â€æ—¶æ‰ä¼šç”Ÿæ•ˆ", "tooltip": "å½“é€‰æ‹©â€œæ‰‹åŠ¨è¾“å…¥â€æ—¶ï¼Œæ­¤å¤„ä½œä¸ºsystemæç¤ºè¯ä½¿ç”¨"}),
                "user_prompt": ("STRING", {"multiline": True, "default": "éšæœºæç¤ºè¯"}),
                "max_tokens": ("INT", {"default": 2048, "min": 1, "max": 8192, "step": 1}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 2.0, "step": 0.05}),
                "top_p": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.05}),
                "control_after_generate": (["å›ºå®š", "å¢åŠ ", "å‡å°‘", "éšæœº"], {"default": "å›ºå®š", "tooltip": "å›ºå®š: ä½¿ç”¨ä¸Šä¸€è½®ç¼“å­˜ç»“æœï¼›å…¶ä»–: æ­£å¸¸ç”Ÿæˆ"}),
            },
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("context", "result")
    FUNCTION = "enhance"
    CATEGORY = "ğŸ¦‰FreeAPI/Prompt Enhance"

    def __init__(self):
        self._cached_map_info: Optional[Dict[str, Dict[str, str]]] = None
        self._cached_templates: Optional[Dict[str, str]] = None
        self._last_reload_ts: float = 0.0
        # ç¼“å­˜ä¸Šä¸€è½®å¢å¼ºç»“æœï¼ˆä»…åœ¨ control_after_generate=å›ºå®š æ—¶å¤ç”¨ï¼‰
        self._last_result: Optional[str] = None

    def _maybe_reload(self):
        now = time.time()
        if now - self._last_reload_ts < 2.0:
            return
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # custom_nodes/Comfyui_Free_API
        config_path = os.path.join(base_dir, "config.json")
        templates_path = os.path.join(os.path.dirname(__file__), "llm_sys_prompt.json")

        config = _load_config(config_path)
        _, map_info = _build_llm_options(config)
        self._cached_map_info = map_info or {}

        self._cached_templates = _load_sys_templates(templates_path)
        self._last_reload_ts = now

    def enhance(
        self,
        llm_model: str,
        preset_template: str,
        sys_prompt: str,
        user_prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.8,
        top_p: float = 0.6,
        control_after_generate: str = "å›ºå®š",
    ):
        """
        ä¸»æ‰§è¡Œé€»è¾‘:
        1) è§£æ llm_model -> base_url, api_key, model
        2) é€‰æ‹©ç³»ç»Ÿæ¨¡ç‰ˆæˆ–æ‰‹åŠ¨ç³»ç»Ÿæç¤ºè¯ï¼Œæ‹¼è£…æœ€ç»ˆæäº¤çš„ system + user
        3) è¯·æ±‚ OpenAI å…¼å®¹æ¥å£ï¼Œå¾—åˆ°ç»“æœ
        4) è¿”å› context ä¸ result
        """
        self._maybe_reload()

        user_prompt = (user_prompt or "").strip()

        map_info = self._cached_map_info or {}
        model_info = map_info.get(str(llm_model).replace("ï¼š", ":"))
        if not model_info:
            context = f"[System] æ— æ³•æ‰¾åˆ°æ¨¡å‹æ˜ å°„: {llm_model}\n[User] {user_prompt}"
            return (context, "æ¨¡å‹æœªé…ç½®æˆ–é…ç½®è¯»å–å¤±è´¥")

        base_url = (model_info.get("base_url") or "").strip()
        api_key = (model_info.get("api_key") or "").strip()
        model = (model_info.get("model") or "").strip()

        sys_templates = self._cached_templates or _load_sys_templates(
            os.path.join(os.path.dirname(__file__), "llm_sys_prompt.json")
        )

        manual_mode = str(preset_template).strip() == "æ‰‹åŠ¨è¾“å…¥"
        if manual_mode:
            system_prompt = (sys_prompt or "").strip()
        else:
            system_prompt = sys_templates.get(preset_template) or sys_templates.get("é€šç”¨å¢å¼º:å›¾æ–‡æç¤ºè¯æ¶¦è‰²") or ""

        context = f"[System]\n{system_prompt}\n\n[User]\n{user_prompt}"

        if not base_url or not model:
            return (context, "base_url æˆ– model ä¸ºç©ºï¼Œè¯·æ£€æŸ¥é…ç½®")

        # å½“é€‰æ‹©â€œå›ºå®šâ€å¹¶ä¸”å·²æœ‰ç¼“å­˜æ—¶ï¼Œç›´æ¥è¿”å›ç¼“å­˜ï¼Œä¸å†è¯·æ±‚API
        if str(control_after_generate).strip() == "å›ºå®š" and isinstance(self._last_result, str) and self._last_result:
            print("[LLM Prompt Enhance] control_after_generate=å›ºå®šï¼Œå·²è·³è¿‡APIè¯·æ±‚ï¼Œå¤ç”¨ä¸Šä¸€è½®ç¼“å­˜ç»“æœã€‚")
            return (context, self._last_result)

        ok, result = _chat_completions(
            base_url=base_url,
            api_key=api_key,
            model=model,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_tokens=int(max_tokens),
            temperature=float(temperature),
            top_p=float(top_p),
        )

        result = result if ok else f"è¯·æ±‚å¤±è´¥: {result}"
        # ä»…å½“è¯·æ±‚æˆåŠŸæ—¶æ›´æ–°ç¼“å­˜
        if ok:
            self._last_result = result
        return (context, result)


# ComfyUI èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "LLM_Prompt_Enhance_Node": LLM_Prompt_Enhance_Node
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "LLM_Prompt_Enhance_Node": "ğŸ¦‰LLM Prompt Enhance"
}
