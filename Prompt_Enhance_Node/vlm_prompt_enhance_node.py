# -*- coding: utf-8 -*-
"""
VLM Prompt Enhance Node

åŠŸèƒ½:
- ä» ComfyUI/custom_nodes/Comfyui_Free_API/config.json çš„ "VLM" æ®µè¯»å–æä¾›æ–¹ã€base_urlã€api_keyã€model åˆ—è¡¨
- åœ¨ ComfyUI ä¸­æä¾›ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå…è®¸ç”¨æˆ·é€‰æ‹© "vlm_model"ï¼ˆå±•ç¤ºä¸º "æä¾›æ–¹:å‹å·"ï¼‰
- è¯»å– Prompt_Enhance_Node/vlm_sys_prompt .json ä¸­çš„ç³»ç»Ÿæç¤ºè¯æ¨¡ç‰ˆåç§°ä½œä¸ºä¸‹æ‹‰é€‰é¡¹ï¼ˆè‹¥æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œåˆ™æä¾›å†…ç½®é»˜è®¤æ¨¡ç‰ˆï¼‰
- æ¥æ”¶ç”¨æˆ· user_promptã€image1ã€image2ã€max_tokensã€temperatureã€top_pï¼›å½“ preset_template é€‰æ‹©â€œæ‰‹åŠ¨è¾“å…¥â€æ—¶ï¼Œä½¿ç”¨ sys_prompt æ–‡æœ¬æ¡†å†…å®¹ä½œä¸ºç³»ç»Ÿæç¤ºè¯
- æŒ‰ OpenAI å…¼å®¹ Chat Completions æ¥å£æ ¼å¼è°ƒç”¨å¯¹åº” base_urlï¼ˆéœ€ /chat/completionsï¼‰ï¼Œæ”¯æŒå›¾æ–‡è¾“å…¥è¯†åˆ«/åæ¨ç­‰æ“ä½œï¼ˆå¯é›¶å›¾æˆ–å¤šå›¾ï¼‰
- è¾“å‡º:
  1) input_prompt: å±•ç¤ºæœ€ç»ˆæäº¤ç»™ API çš„ system_prompt + user_promptï¼ˆä¾¿äºæ£€æŸ¥ä¸Šä¸‹æ–‡ï¼‰
  2) output_prompt: API å“åº”ç”Ÿæˆå†…å®¹

æ³¨æ„:
- æœ¬å®ç°ä»…ç”¨äºåˆæ³•ã€æ­£å½“çš„æç¤ºè¯ä¼˜åŒ–/å›¾æ–‡ç†è§£ï¼Œä¸åŒ…å«ä»»ä½•æ¶æ„ç”¨é€”ã€‚
"""

import json
import os
import re
import time
import io
import base64
from typing import Dict, Any, List, Tuple, Optional

try:
    import requests
except Exception:
    requests = None

# å¯é€‰ä¾èµ–
try:
    import numpy as np
except Exception:
    np = None

try:
    from PIL import Image
except Exception:
    Image = None


def _load_config(config_path: str) -> Dict[str, Any]:
    try:
        if not os.path.exists(config_path):
            return {}
        with open(config_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if not content:
                return {}
            return json.loads(content)
    except Exception:
        return {}


def _load_sys_templates(templates_path: str) -> Dict[str, str]:
    """
    æœŸæœ›æ–‡ä»¶æ ¼å¼:
    {
      "æ¨¡æ¿ä¸­æ–‡åA": "è¿™é‡Œæ˜¯system promptå†…å®¹A",
      "æ¨¡æ¿ä¸­æ–‡åB": "è¿™é‡Œæ˜¯system promptå†…å®¹B"
    }
    è‹¥æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›é»˜è®¤æ¨¡ç‰ˆé›†åˆã€‚
    """
    defaults = {
        "å›¾æ–‡ç†è§£:ç²¾ç¡®æè¿°": (
            "ä½ æ˜¯èµ„æ·±è§†è§‰ç†è§£ä¸“å®¶ã€‚è¯·å¯¹è¾“å…¥å›¾åƒè¿›è¡Œå®¢è§‚ã€å…·ä½“çš„è‹±æ–‡æè¿°ï¼Œè¦†ç›–ä¸»ä½“ã€åœºæ™¯ã€æ„å›¾ã€å…‰çº¿ã€é¢œè‰²ã€æè´¨ã€æ–‡å­—ä¿¡æ¯ã€åŠ¨ä½œä¸å…³ç³»ï¼›"
            "é¿å…è‡†æµ‹ä¸å¯è§ä¿¡æ¯ï¼Œé¿å…å†—ä½™ï¼Œé‡è¦ä¿¡æ¯é å‰ã€‚ä»…è¾“å‡ºè‹±æ–‡æè¿°ã€‚"
        ),
        "åæ¨æç¤ºè¯:StableDiffusioné£æ ¼": (
            "ä½ æ˜¯æç¤ºè¯åæ¨å·¥ç¨‹å¸ˆã€‚è¯·åŸºäºå›¾åƒå†…å®¹äº§å‡ºé€‚ç”¨äºå›¾åƒç”Ÿæˆæ¨¡å‹çš„è‹±æ–‡æç¤ºè¯ï¼ŒåŒ…å«ä¸»ä½“ã€é£æ ¼ã€é•œå¤´ã€å…‰æ•ˆã€æ„å›¾ã€æè´¨ã€åˆ†è¾¨ç‡å…³é”®è¯ï¼Œ"
            "å¹¶æä¾›å¿…è¦çš„è´Ÿé¢æç¤ºè¯ï¼ˆnegative promptsï¼‰ã€‚ä»…è¾“å‡ºæœ€ç»ˆæç¤ºè¯æ–‡æœ¬ã€‚"
        ),
        "OCRä¸å¸ƒå±€:è¦ç‚¹æå–": (
            "ä½ æ˜¯OCRä¸ç‰ˆé¢åˆ†æåŠ©ç†ã€‚è¯†åˆ«å›¾ä¸­æ–‡å­—ä¸å…³é”®å…ƒç´ çš„ç©ºé—´å¸ƒå±€ï¼ˆå·¦/å³/ä¸Š/ä¸‹ã€ç›¸é‚»å…³ç³»ï¼‰ï¼Œ"
            "ä»¥ç»“æ„åŒ–è‹±æ–‡æ®µè½è¾“å‡ºï¼Œä¸è¾“å‡ºå¤šä½™è§£é‡Šã€‚"
        ),
    }
    try:
        if not os.path.exists(templates_path):
            return defaults
        with open(templates_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if not content:
                return defaults
            data = json.loads(content)
            if isinstance(data, dict) and data:
                cleaned = {str(k): str(v) for k, v in data.items() if isinstance(k, str)}
                return cleaned or defaults
            return defaults
    except Exception:
        return defaults


def _build_vlm_options(config: Dict[str, Any]) -> Tuple[List[str], Dict[str, Dict[str, str]]]:
    """
    è¿”å›:
    - options: ["æä¾›æ–¹:å‹å·", ...]
    - map_info: {"æä¾›æ–¹:å‹å·": {"base_url": "...", "api_key": "...", "model": "å‹å·"}}
    """
    options: List[str] = []
    map_info: Dict[str, Dict[str, str]] = {}
    if not config:
        return options, map_info

    vlm = config.get("VLM", {})
    for provider, info in vlm.items():
        try:
            base_url = str(info.get("base_url", "")).strip()
            api_key = str(info.get("api_key", "")).strip()
            models = info.get("model", [])
            if not isinstance(models, list):
                continue
            for m in models:
                model_name = str(m).strip()
                label = f"{provider}:{model_name}".replace("ï¼š", ":")
                options.append(label)
                map_info[label] = {"base_url": base_url, "api_key": api_key, "model": model_name}
        except Exception:
            continue
    return options, map_info


def _normalize_base_url(url: str) -> str:
    u = url.strip()
    u = re.sub(r"\s+", " ", u).strip()
    u = u.rstrip("/")
    return u


def _image_tensor_to_data_url(img) -> Optional[str]:
    """
    å°†ComfyUIçš„IMAGEå¼ é‡è½¬ä¸º data:image/png;base64,... å­—ç¬¦ä¸²
    æ”¯æŒ: torch.Tensor æˆ– numpy.ndarray æˆ– PIL.Image
    """
    try:
        if img is None:
            return None
        try:
            import torch
        except Exception:
            torch = None

        array = None
        if torch is not None and isinstance(img, torch.Tensor):
            t = img
            if t.dim() == 4:
                t = t[0]
            if t.shape[0] in (3, 4):
                t = t.permute(1, 2, 0)
            t = t.detach().cpu().float().clamp(0, 1).numpy()
            array = (t * 255.0).round().astype("uint8")
        elif np is not None and isinstance(img, np.ndarray):
            array = img
            if array.dtype != np.uint8:
                array = (np.clip(array, 0, 1) * 255.0).round().astype("uint8")
        elif Image is not None and isinstance(img, Image.Image):
            pass
        else:
            return None

        if Image is None:
            return None

        if isinstance(img, Image.Image):
            pil = img
        else:
            if array.ndim == 2:
                mode = "L"
            elif array.shape[2] == 4:
                mode = "RGBA"
            else:
                mode = "RGB"
            pil = Image.fromarray(array, mode=mode)

        buf = io.BytesIO()
        pil.save(buf, format="PNG")
        b64 = base64.b64encode(buf.getvalue()).decode("ascii")
        return f"data:image/png;base64,{b64}"
    except Exception:
        return None


def _chat_completions_with_images(
    base_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    image_urls: List[str],
    max_tokens: int,
    temperature: float,
    top_p: float,
    timeout: int = 60,
) -> Tuple[bool, str]:
    """
    OpenAI å…¼å®¹æ¥å£ï¼Œå›¾æ–‡è¾“å…¥:
      POST {base_url}/chat/completions
    messages:
      - system: system_prompt
      - user: [{"type":"text","text": ...}, {"type":"image_url","image_url":{"url": ...}}, ...]
    """
    if requests is None:
        return False, "requests åº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…åé‡è¯•ã€‚"

    try:
        url = _normalize_base_url(base_url) + "/chat/completions"
        headers = {"Content-Type": "application/json"}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"

        user_content = []
        if user_prompt:
            user_content.append({"type": "text", "text": user_prompt})
        for u in image_urls:
            user_content.append({"type": "image_url", "image_url": {"url": u}})

        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content if user_content else user_prompt},
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "stream": False,
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)
        if resp.status_code >= 400:
            return False, f"HTTP {resp.status_code}: {resp.text}"

        data = resp.json()
        if isinstance(data, dict):
            choices = data.get("choices") or []
            if isinstance(choices, list) and choices:
                msg = choices[0].get("message") or {}
                content = msg.get("content")
                if isinstance(content, str) and content.strip():
                    return True, content.strip()
                for key in ["reasoning_content", "text", "output", "answer"]:
                    val = msg.get(key)
                    if isinstance(val, str) and val.strip():
                        return True, val.strip()
            for key in ["output", "answer", "text", "message"]:
                v = data.get(key)
                if isinstance(v, str) and v.strip():
                    return True, v.strip()
        return False, json.dumps(data, ensure_ascii=False)
    except Exception as e:
        return False, f"Exception: {e}"


class VLM_Prompt_Enhance_Node:
    """
    ComfyUI èŠ‚ç‚¹å®šä¹‰:
    - è¾“å…¥:
        1) vlm_model: ä¸‹æ‹‰ï¼Œæ¥è‡ª config.json çš„ VLM ç»„åˆé¡¹ ("æä¾›æ–¹:å‹å·")
        2) preset_template: ä¸‹æ‹‰ï¼Œé¦–ä½ä¸ºâ€œæ‰‹åŠ¨è¾“å…¥â€ï¼Œå…¶ä½™æ¥è‡ª vlm_sys_prompt .jsonï¼›å¦‚æœæ–‡ä»¶ä¸ºç©ºåˆ™æä¾›å†…ç½®æ¨¡ç‰ˆ
        3) sys_prompt: æ–‡æœ¬æ¡†ï¼ˆsys_promptï¼Œè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ä»…åœ¨é¢„è®¾æ¨¡æ¿ä¸ºâ€œæ‰‹åŠ¨è¾“å…¥â€æ—¶æ‰ç”Ÿæ•ˆï¼‰
        4) user_prompt: æ–‡æœ¬æ¡†
        5) max_tokens: æ•´æ•°ï¼ˆé»˜è®¤ 2048ï¼‰
        6) temperature: æµ®ç‚¹ï¼ˆé»˜è®¤ 0.8ï¼‰
        7) top_p: æµ®ç‚¹ï¼ˆé»˜è®¤ 0.6ï¼‰
    - å¯é€‰:
        8) image1: IMAGEï¼ˆå¯é€‰ï¼‰
        9) image2: IMAGEï¼ˆå¯é€‰ï¼‰
    - è¾“å‡º:
        1) input_prompt: strï¼ˆsystem + user æœ€ç»ˆæäº¤å†…å®¹ï¼‰
        2) output_prompt: strï¼ˆAPI å“åº”å¢å¼ºç»“æœï¼‰
    """

    @classmethod
    def INPUT_TYPES(cls):
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # custom_nodes/Comfyui_Free_API
        config_path = os.path.join(base_dir, "config.json")
        templates_path = os.path.join(os.path.dirname(__file__), "vlm_sys_prompt .json")

        config = _load_config(config_path)
        vlm_options, _ = _build_vlm_options(config)
        if not vlm_options:
            vlm_options = []

        sys_templates = _load_sys_templates(templates_path)
        template_names = ["æ‰‹åŠ¨è¾“å…¥"] + (list(sys_templates.keys()) if sys_templates else ["å›¾æ–‡ç†è§£:ç²¾ç¡®æè¿°"])

        return {
            "required": {
                "vlm_model": (vlm_options if vlm_options else ["æœªæ£€æµ‹åˆ°æ¨¡å‹é…ç½®"],),
                "preset_template": (template_names,),
                "sys_prompt": ("STRING", {"multiline": True, "default": "è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ä»…åœ¨é¢„è®¾æ¨¡æ¿ä¸ºâ€œæ‰‹åŠ¨è¾“å…¥â€æ—¶æ‰ä¼šç”Ÿæ•ˆ", "tooltip": "å½“é€‰æ‹©â€œæ‰‹åŠ¨è¾“å…¥â€æ—¶ï¼Œæ­¤å¤„ä½œä¸ºsystemæç¤ºè¯ä½¿ç”¨"}),
                "user_prompt": ("STRING", {"multiline": True, "default": "æ— éœ€ä»»ä½•è§£é‡Šå’Œé“ºå«ï¼Œç›´æ¥è¾“å‡ºç»“æœ"}),
                "max_tokens": ("INT", {"default": 2048, "min": 1, "max": 8192, "step": 1}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 2.0, "step": 0.05}),
                "top_p": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.05}),
            },
            "optional": {
                "image1": ("IMAGE",),
                "image2": ("IMAGE",),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("input_prompt", "output_prompt")
    FUNCTION = "enhance"
    CATEGORY = "ğŸ¦‰FreeAPI/Prompt Enhance"

    def __init__(self):
        self._cached_map_info: Optional[Dict[str, Dict[str, str]]] = None
        self._cached_templates: Optional[Dict[str, str]] = None
        self._last_reload_ts: float = 0.0

    def _maybe_reload(self):
        now = time.time()
        if now - self._last_reload_ts < 2.0:
            return
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # custom_nodes/Comfyui_Free_API
        config_path = os.path.join(base_dir, "config.json")
        templates_path = os.path.join(os.path.dirname(__file__), "vlm_sys_prompt .json")

        config = _load_config(config_path)
        _, map_info = _build_vlm_options(config)
        self._cached_map_info = map_info or {}

        self._cached_templates = _load_sys_templates(templates_path)
        self._last_reload_ts = now

    def enhance(
        self,
        vlm_model: str,
        preset_template: str,
        sys_prompt: str,
        user_prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.8,
        top_p: float = 0.6,
        image1=None,
        image2=None,
    ):
        """
        ä¸»æ‰§è¡Œé€»è¾‘:
        1) è§£æ vlm_model -> base_url, api_key, model
        2) é€‰æ‹©ç³»ç»Ÿæ¨¡ç‰ˆæˆ–æ‰‹åŠ¨ç³»ç»Ÿæç¤ºè¯ï¼Œæ‹¼è£…æœ€ç»ˆæäº¤çš„ system + user + images(0~2å¼ )
        3) è¯·æ±‚ OpenAI å…¼å®¹æ¥å£ï¼Œå¾—åˆ°ç»“æœ
        4) è¿”å› input_prompt ä¸ output_prompt
        """
        self._maybe_reload()

        user_prompt = (user_prompt or "").strip()

        # å¤„ç†å›¾ç‰‡ -> data url åˆ—è¡¨
        image_urls: List[str] = []
        for img in (image1, image2):
            data_url = _image_tensor_to_data_url(img)
            if data_url:
                image_urls.append(data_url)

        map_info = self._cached_map_info or {}
        model_info = map_info.get(str(vlm_model).replace("ï¼š", ":"))
        if not model_info:
            input_prompt = f"[System] æ— æ³•æ‰¾åˆ°æ¨¡å‹æ˜ å°„: {vlm_model}\n[User] {user_prompt}\n[Images] {len(image_urls)}"
            return (input_prompt, "æ¨¡å‹æœªé…ç½®æˆ–é…ç½®è¯»å–å¤±è´¥")

        base_url = (model_info.get("base_url") or "").strip()
        api_key = (model_info.get("api_key") or "").strip()
        model = (model_info.get("model") or "").strip()

        sys_templates = self._cached_templates or _load_sys_templates(
            os.path.join(os.path.dirname(__file__), "vlm_sys_prompt .json")
        )

        manual_mode = str(preset_template).strip() == "æ‰‹åŠ¨è¾“å…¥"
        if manual_mode:
            system_prompt = (sys_prompt or "").strip()
        else:
            system_prompt = sys_templates.get(preset_template) or sys_templates.get("å›¾æ–‡ç†è§£:ç²¾ç¡®æè¿°") or ""

        input_prompt = f"[System]\n{system_prompt}\n\n[User]\n{user_prompt}\n\n[Images]\n{len(image_urls)} image(s)"

        if not base_url or not model:
            return (input_prompt, "base_url æˆ– model ä¸ºç©ºï¼Œè¯·æ£€æŸ¥é…ç½®")

        ok, result = _chat_completions_with_images(
            base_url=base_url,
            api_key=api_key,
            model=model,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            image_urls=image_urls,
            max_tokens=int(max_tokens),
            temperature=float(temperature),
            top_p=float(top_p),
        )

        output_prompt = result if ok else f"è¯·æ±‚å¤±è´¥: {result}"
        return (input_prompt, output_prompt)


# ComfyUI èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "VLM_Prompt_Enhance_Node": VLM_Prompt_Enhance_Node
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "VLM_Prompt_Enhance_Node": "ğŸ¦‰VLM Prompt Enhance"
}