import os
import json
import base64
import requests
from PIL import Image
from io import BytesIO
import re

# èŠ‚ç‚¹ä¸»ç±»
class GeminiVLMAPI:
    """
    ComfyUIè‡ªå®šä¹‰èŠ‚ç‚¹ï¼šGemini VLM API
    å®ç°å›¾ç‰‡è§†è§‰æ¨ç†APIè°ƒç”¨ï¼Œä¸“é—¨é’ˆå¯¹Gemini APIï¼Œå‚æ•°è‡ªåŠ¨è¯»å–config.jsonã€‚
    è¾“å…¥å‚æ•°ï¼šimage, model, max_tokens, temperature, top_p, system_prompt, user_prompt
    è¾“å‡ºï¼šanswerï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰, reasoning_contentï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰, tokens_usageï¼ˆAPIç”¨é‡ä¿¡æ¯ï¼‰
    """
    def __init__(self):
        # è¯»å–é…ç½®æ–‡ä»¶ï¼Œå®¹é”™åŒ¹é… VLM ä¸‹çš„æä¾›æ–¹ï¼ˆä¼˜å…ˆâ€œè°·æ­Œå®˜æ–¹â€ï¼Œå¦åˆ™å›é€€åˆ°ç¬¬ä¸€ä¸ªå« model åˆ—è¡¨çš„æä¾›æ–¹ï¼‰
        config_path = os.path.join(os.path.dirname(__file__), '..', 'config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            cfg_all = json.load(f)
        vlm = cfg_all.get('VLM', {}) or {}
        provider_key = None
        if "è°·æ­Œå®˜æ–¹" in vlm and isinstance(vlm["è°·æ­Œå®˜æ–¹"], dict):
            provider_key = "è°·æ­Œå®˜æ–¹"
        else:
            for k, v in vlm.items():
                if isinstance(v, dict) and isinstance(v.get('model'), list):
                    provider_key = k
                    break
        self.config = vlm.get(provider_key, {})

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€è¯»å–æ¨¡å‹é€‰é¡¹ï¼ˆå®¹é”™åŒ¹é…â€œè°·æ­Œå®˜æ–¹â€ï¼Œå¦åˆ™å›é€€åˆ°ç¬¬ä¸€ä¸ªåŒ…å« model åˆ—è¡¨çš„æä¾›æ–¹ï¼‰
        config_path = os.path.join(os.path.dirname(__file__), '..', 'config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            cfg_all = json.load(f)
        vlm = cfg_all.get('VLM', {}) or {}
        selected = {}
        if "è°·æ­Œå®˜æ–¹" in vlm and isinstance(vlm["è°·æ­Œå®˜æ–¹"], dict):
            selected = vlm["è°·æ­Œå®˜æ–¹"]
        else:
            for k, v in vlm.items():
                if isinstance(v, dict) and isinstance(v.get('model'), list):
                    selected = v
                    break
        model_options = selected.get('model', ['gemini-2.5-flash'])
        return {
            "required": {
                "image": ("IMAGE",),
                "model": (model_options, {"default": model_options[0]}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 16384, "step": 1}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 2.0, "step": 0.01}),
                "top_p": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.01}),
                "system_prompt": ("STRING", {"multiline": True, "default": "ä½ æ˜¯ä¸€ä¸ªèƒ½åˆ†æå›¾åƒçš„AIåŠ©æ‰‹ã€‚è¯·ä»”ç»†è§‚å¯Ÿå›¾åƒï¼Œå¹¶æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›è¯¦ç»†ã€å‡†ç¡®çš„æè¿°ã€‚"}),
                "user_prompt": ("STRING", {"multiline": True, "default": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚"}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("reasoning_content", "answer", "tokens_usage")
    FUNCTION = "infer"
    CATEGORY = "ğŸ¦‰FreeAPI/Gemini"

    def infer(self, image, model, max_tokens, temperature, top_p, system_prompt, user_prompt):
        """
        ä¸»æ¨ç†æ–¹æ³•ï¼š
        1. å°†imageè½¬ä¸ºbase64
        2. æ„é€ Gemini APIè¯·æ±‚
        3. å‘é€è¯·æ±‚ï¼Œè¿”å›æ–‡æœ¬
        4. è§£æreasoning_contentå’Œanswer
        """
        # è¯»å–Gemini APIå‚æ•°
        base_url = self.config.get('base_url', 'https://generativelanguage.googleapis.com/v1beta/openai/')
        api_key = self.config.get('api_key', '')
        
        if not api_key:
            return ("", "", "é”™è¯¯ï¼šæœªé…ç½®Gemini API Keyï¼Œè¯·åœ¨config.jsonçš„ VLM éƒ¨åˆ†å¯¹åº”æä¾›æ–¹ä¸‹è®¾ç½® api_keyï¼ˆä¾‹å¦‚â€œè°·æ­Œå®˜æ–¹â€.api_keyï¼‰")
        
        # 1. å›¾ç‰‡è½¬base64
        try:
            # ComfyUIçš„IMAGEæ˜¯torch.Tensorï¼Œéœ€è¦è½¬æ¢ä¸ºPIL Image
            if hasattr(image, 'cpu'):  # æ˜¯torch.Tensor
                # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œç„¶åè½¬ä¸ºPIL Image
                import torch
                if image.dim() == 4:  # batchç»´åº¦ï¼Œå–ç¬¬ä¸€å¼ 
                    image = image[0]
                # è½¬æ¢ä¸ºnumpyå¹¶è°ƒæ•´é€šé“é¡ºåº (C,H,W) -> (H,W,C)
                image_np = image.cpu().numpy()
                if image_np.shape[0] == 3:  # å¦‚æœæ˜¯(C,H,W)æ ¼å¼
                    image_np = image_np.transpose(1, 2, 0)
                # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
                image_np = (image_np * 255).clip(0, 255).astype('uint8')
                img = Image.fromarray(image_np)
            else:
                # å¦‚æœä¸æ˜¯tensorï¼Œç›´æ¥ä½¿ç”¨
                img = image
            
            output_buffer = BytesIO()
            img.save(output_buffer, format="JPEG")
            image_data_bytes_jpeg = output_buffer.getvalue()
            image_base64 = base64.b64encode(image_data_bytes_jpeg).decode('utf-8')
        except Exception as e:
            return ("", f"å›¾ç‰‡å¤„ç†å¤±è´¥: {e}", "")
        
        # 2. æ„é€ Gemini APIè¯·æ±‚
        messages = []
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })
        messages.append({
            "role": "user",
            "content": [
                {"type": "text", "text": user_prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
            ]
        })
        
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p
        }
        
        # 3. å‘é€è¯·æ±‚
        try:
            headers = self._build_headers(api_key)
            # ç¡®ä¿URLæ ¼å¼æ­£ç¡®ï¼Œé¿å…åŒæ–œæ 
            api_url = f"{base_url.rstrip('/')}/chat/completions"
            print(f"æ­£åœ¨è¯·æ±‚Gemini API: {api_url}")
            print(f"è¯·æ±‚å‚æ•°: model={model}, max_tokens={max_tokens}")
            
            resp = requests.post(api_url, headers=headers, json=payload, timeout=120)
            resp.raise_for_status()
            
            return self._parse_response(resp)
                
        except requests.exceptions.ConnectTimeout as e:
            return ("", f"ç½‘ç»œè¿æ¥è¶…æ—¶: æ— æ³•è¿æ¥åˆ°Gemini APIæœåŠ¡å™¨ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä½¿ç”¨ä»£ç†ã€‚é”™è¯¯: {e}", "")
        except requests.exceptions.Timeout as e:
            return ("", f"è¯·æ±‚è¶…æ—¶: APIå“åº”æ—¶é—´è¿‡é•¿ã€‚è¯·ç¨åé‡è¯•æˆ–å‡å°‘max_tokensã€‚é”™è¯¯: {e}", "")
        except requests.exceptions.ConnectionError as e:
            return ("", f"ç½‘ç»œè¿æ¥é”™è¯¯: æ— æ³•å»ºç«‹åˆ°Gemini APIçš„è¿æ¥ã€‚è¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®ã€‚é”™è¯¯: {e}", "")
        except requests.exceptions.RequestException as e:
            return ("", f"APIè¯·æ±‚å¤±è´¥: {e}", "")
        except Exception as e:
            return ("", f"å¤„ç†å¤±è´¥: {e}", "")

    def _parse_response(self, resp):
        """
        è§£æGeminiå“åº”ï¼Œå°è¯•æå–æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ
        å‚è€ƒLLM_Partyæ’ä»¶çš„è§£æé€»è¾‘
        """
        try:
            data = resp.json()
            print("Gemini APIåŸå§‹å“åº”:", data)  # è°ƒè¯•è¾“å‡º
            usage = data.get("usage", {})
            tokens_usage = self._format_tokens_usage(usage)
            if "choices" in data and data["choices"]:
                message = data["choices"][0]["message"]
                content = message.get("content", "")
                finish_reason = data["choices"][0].get("finish_reason", "")
                if not content:
                    # æ–°å¢ï¼šå¦‚æœå†…å®¹ä¸ºç©ºï¼Œè¾“å‡ºå‹å¥½æç¤º
                    return ("", f"æœªè¿”å›å†…å®¹ï¼Œfinish_reason={finish_reason}", tokens_usage)
                reasoning_content = message.get("reasoning_content", None)
                if reasoning_content is None:
                    reasoning_content, answer = self._parse_content_tags(content)
                else:
                    answer = content
                return (reasoning_content or "", answer, tokens_usage)
            else:
                return ("", "APIæœªè¿”å›choiceså†…å®¹", tokens_usage)
        except Exception as e:
            return ("", f"å“åº”è§£æå¤±è´¥: {e}", "")

    def _parse_content_tags(self, content):
        """
        è§£æGeminiå“åº”ï¼Œå°è¯•æå–æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ
        å‚è€ƒLLM_Partyæ’ä»¶çš„è§£æé€»è¾‘
        """
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–<think>æ ‡ç­¾å†…å®¹
            pattern = r'<think>(.*?)</think>'
            match = re.search(pattern, content, re.DOTALL)
            
            if match:
                # æ‰¾åˆ°<think>æ ‡ç­¾ï¼Œæå–æ€è€ƒè¿‡ç¨‹
                reasoning_content = match.group(1).strip()
                # ç§»é™¤<think>æ ‡ç­¾ï¼Œå‰©ä½™å†…å®¹ä½œä¸ºç­”æ¡ˆ
                answer = content.replace(match.group(0), "").strip()
            else:
                # æ²¡æœ‰<think>æ ‡ç­¾ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                # å°è¯•æå–<answer>æ ‡ç­¾
                answer_match = re.search(r'<answer>(.*?)</answer>', content, re.DOTALL)
                if answer_match:
                    answer = answer_match.group(1).strip()
                    reasoning_content = ""
                else:
                    # å°è¯•æå–<answer>åé¢çš„å†…å®¹
                    answer_match = re.search(r'<answer>(.*)', content, re.DOTALL)
                    if answer_match:
                        answer = answer_match.group(1).strip()
                        reasoning_content = ""
                    else:
                        # æ²¡æœ‰ç‰¹æ®Šæ ‡ç­¾ï¼Œæ•´ä¸ªå†…å®¹ä½œä¸ºç­”æ¡ˆ
                        answer = content.strip()
                        reasoning_content = ""
            
            return (reasoning_content, answer)
            
        except Exception as e:
            # è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹ä½œä¸ºç­”æ¡ˆ
            return ("", content.strip())

    def _format_tokens_usage(self, usage):
        """
        å°†tokens_usageæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²
        """
        if not usage:
            return ""
        return f"total_tokens={usage.get('total_tokens', '-')}, input_tokens={usage.get('prompt_tokens', '-')}, output_tokens={usage.get('completion_tokens', '-')}"

    def _build_headers(self, api_key):
        # Geminiä½¿ç”¨Bearer tokenè®¤è¯
        return {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "Gemini_VLM_API": GeminiVLMAPI
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "Gemini_VLM_API": "ğŸ¦‰Gemini VLM APIèŠ‚ç‚¹"
} 